{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/siva/Documents/thesis/datasets/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/siva/Documents/thesis/datasets/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/siva/Documents/thesis/datasets/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/siva/Documents/thesis/datasets/MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/Users/siva/Documents/thesis/datasets/MNIST\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_(shape):\n",
    "    return tf.Variable(tf.random_normal(shape,stddev = 0.01))\n",
    "    #return tf.Variable(tf.constant(2.0,shape = shape))\n",
    "def biases_(shape):\n",
    "    return tf.Variable(tf.random_normal(shape,stddev = 0.01))\n",
    "def conv2d_(inp_,w_):\n",
    "    return tf.nn.conv2d(inp_,w_,strides = [1,1,1,1],padding = \"SAME\")\n",
    "def unpickle(file_name):\n",
    "    with open(file_name,\"rb\") as fo:\n",
    "        dict_ = pickle.load(fo,encoding = \"bytes\")\n",
    "        temp = dict_[b'data'][:]\n",
    "        dict_[b'data'] = np.reshape(temp,(-1,32,32,3))\n",
    "    return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_final(FILTER_SIZE,MAX_FILTERS,RATE_OF_NEURONS,LEARNING_RATE,INPUT_DIM = [None,28,28,1],OUTPUT_DIM = [None,10]):\n",
    "    \"\"\"\n",
    "    FILTER_SIZE: size of the filters used in convolution\n",
    "    MAX_FILTERS: maximum number of filters initialized\n",
    "    RATE_OF_NEURONS: rate of increase of neurons\n",
    "    LEARNING_RATE: learning rate of the optimizer\n",
    "    INPUT_DIM: input dimension of placeholder x\n",
    "    OUTPUT_DIM: output dimension of placeholder y\n",
    "    \"\"\"\n",
    "    x = tf.placeholder(tf.float32,INPUT_DIM)\n",
    "    y = tf.placeholder(tf.float32,OUTPUT_DIM)\n",
    "    \n",
    "    no_of_filters = tf.placeholder(tf.int32)\n",
    "    mask = tf.placeholder(tf.int32)\n",
    "    \n",
    "    conv_w_original = weights_([FILTER_SIZE,FILTER_SIZE,3,MAX_FILTERS])\n",
    "    conv_b_original = biases_([MAX_FILTERS])\n",
    "    \n",
    "    #conv_w = tf.slice(conv_w_original,(0,0,0,0),(FILTER_SIZE,FILTER_SIZE,tf.shape(x)[-1],no_of_filters))\n",
    "    #conv_b = tf.slice(conv_b_original,(0),(no_of_filters))\n",
    "    conv_w = conv_w_original[0:FILTER_SIZE,0:FILTER_SIZE,0:tf.shape(x)[-1],0:no_of_filters]\n",
    "    conv_b = conv_b_original[0:no_of_filters]\n",
    "    \n",
    "    conv_w_mask = tf.concat([tf.ones([FILTER_SIZE,FILTER_SIZE,tf.shape(x)[-1],no_of_filters-RATE_OF_NEURONS]),\\\n",
    "                             tf.zeros([FILTER_SIZE,FILTER_SIZE,tf.shape(x)[-1],RATE_OF_NEURONS])],3)\n",
    "    conv_b_mask = tf.concat([tf.ones([no_of_filters-RATE_OF_NEURONS]),tf.zeros([RATE_OF_NEURONS])],0)\n",
    "    conv_w_mask_not = tf.concat([tf.zeros([FILTER_SIZE,FILTER_SIZE,tf.shape(x)[-1],no_of_filters-RATE_OF_NEURONS]),\\\n",
    "                                 tf.ones([FILTER_SIZE,FILTER_SIZE,tf.shape(x)[-1],RATE_OF_NEURONS])],3)\n",
    "    conv_b_mask_not = tf.concat([tf.zeros([no_of_filters-RATE_OF_NEURONS]),tf.ones([RATE_OF_NEURONS])],0)\n",
    "    \n",
    "    #depending on the mask value gradient update is done\n",
    "    w = tf.cond(tf.equal(mask,1),lambda:tf.stop_gradient(conv_w * conv_w_mask) + (conv_w * conv_w_mask_not),\\\n",
    "                lambda:conv_w)\n",
    "    b = tf.cond(tf.equal(mask,1),lambda:tf.stop_gradient(conv_b * conv_b_mask) + (conv_b * conv_b_mask_not),\\\n",
    "                lambda:conv_b)\n",
    "        \n",
    "    conv_out = tf.nn.relu(tf.nn.bias_add(conv2d_(x,w),b))\n",
    "    \n",
    "    full_w_original = weights_([INPUT_DIM[1]*INPUT_DIM[2]*MAX_FILTERS,10])\n",
    "    #full_w = tf.slice(full_w_original,(0,0),(INPUT_DIM[1]*INPUT_DIM[2]*no_of_filters,10))\n",
    "    full_w = full_w_original[0:(INPUT_DIM[1]*INPUT_DIM[2]*no_of_filters),0:10]\n",
    "    full_b = biases_([10])\n",
    "    \n",
    "    out = tf.matmul(tf.reshape(conv_out,(-1,INPUT_DIM[1]*INPUT_DIM[2]*no_of_filters)),full_w) + full_b\n",
    "    #softmax_out = tf.nn.softmax(out)\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = out,labels = y))\n",
    "    prediction = tf.equal(tf.argmax(out,axis = 1),tf.argmax(y, axis = 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(prediction,tf.float32))\n",
    "    optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "    \n",
    "    #summary writing\n",
    "    tf.summary.scalar(\"loss\",loss)\n",
    "    tf.summary.scalar(\"acc\",accuracy)\n",
    "    tensor_b = tf.summary.merge_all()\n",
    "    \n",
    "    return {\"inputs\":[x,y,no_of_filters,mask],\"weight\":conv_w,\"optimizer\":optimizer,\"loss\":loss,\"output\":conv_out,\"accuracy\":accuracy,\"tensor_b\":tensor_b}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cifar(learning_rate):\n",
    "    DATA_FOLDER = \"/Users/siva/Documents/thesis/datasets/cifar_10/\"\n",
    "    TENSORBOARD_FOLDER = \"/Users/siva/Documents/thesis/tensorboard\"\n",
    "    EPOCHS = 10\n",
    "    BATCH_SIZE = 100\n",
    "    #loading the data\n",
    "    for i in range(5):\n",
    "        if i == 0:\n",
    "            data = unpickle(DATA_FOLDER+\"data_batch_{0}\".format(i+1))[b'data']\n",
    "            labels = unpickle(DATA_FOLDER+\"data_batch_{0}\".format(i+1))[b'labels']\n",
    "            labels = np.array(list(map(lambda x:np.eye(10)[x],labels)))\n",
    "        else:\n",
    "            data = np.append(data,unpickle(DATA_FOLDER+\"data_batch_{0}\".format(i+1))[b'data'],axis = 0)\n",
    "            labels_ = unpickle(DATA_FOLDER+\"data_batch_{0}\".format(i+1))[b'labels']\n",
    "            labels = np.append(labels,np.array(list(map(lambda x:np.eye(10)[x],labels_))),axis = 0)\n",
    "    test_data = unpickle(DATA_FOLDER+\"test_batch\")[b'data']\n",
    "    test_labels = unpickle(DATA_FOLDER+\"test_batch\")[b'labels']\n",
    "    test_labels = np.array(list(map(lambda x:np.eye(10)[x],test_labels)))\n",
    "    print(test_data.shape)\n",
    "    print(test_labels.shape)\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess:\n",
    "        cifar_model = model_final(3,300,1,learning_rate,[None,32,32,3])\n",
    "        filters = 256\n",
    "        mask = 0\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        train_writer = tf.summary.FileWriter(TENSORBOARD_FOLDER+\"/train\",sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(TENSORBOARD_FOLDER+\"/test\",sess.graph)\n",
    "        train_count = 0\n",
    "        test_count = 0\n",
    "        for e in range(EPOCHS):\n",
    "            train_loss_list = []\n",
    "            train_acc_list = []\n",
    "            test_loss_list = []\n",
    "            test_acc_list = []\n",
    "            for b in range(int(data.shape[0]/BATCH_SIZE)):\n",
    "                train_data = data[b:b+BATCH_SIZE,:]\n",
    "                train_labels = labels[b:b+BATCH_SIZE,:]\n",
    "                to_compute = [cifar_model[\"optimizer\"],cifar_model[\"loss\"],cifar_model[\"accuracy\"],cifar_model[\"tensor_b\"]]\n",
    "                feed_dict_ = dict(zip(cifar_model[\"inputs\"],[train_data,train_labels,filters,mask]))\n",
    "                _,train_loss,train_acc,train_summ = sess.run(to_compute,feed_dict = feed_dict_)\n",
    "                train_loss_list.append(train_loss)\n",
    "                train_acc_list.append(train_acc)\n",
    "                #train_writer.add_summary(train_summ,train_count)\n",
    "                train_count += 1\n",
    "            print(\"train:\",np.mean(train_loss_list),\"|\",np.mean(train_acc_list))\n",
    "            for t in range(int(test_data.shape[0]/BATCH_SIZE)):\n",
    "                test_data_ = test_data[t:t+BATCH_SIZE,:]\n",
    "                test_labels_ = test_labels[t:t+BATCH_SIZE,:]\n",
    "                to_compute_t = [cifar_model[\"loss\"],cifar_model[\"accuracy\"],cifar_model[\"tensor_b\"]]\n",
    "                feed_dict_t = dict(zip(cifar_model[\"inputs\"],[test_data_,test_labels_,filters,mask]))\n",
    "                test_loss,test_acc,test_summ = sess.run(to_compute_t,feed_dict = feed_dict_t)\n",
    "                test_loss_list.append(test_loss)\n",
    "                test_acc_list.append(test_acc)\n",
    "                #test_writer.add_summary(test_summ,test_count)\n",
    "                test_count += 1\n",
    "            print(\"test:\",np.mean(test_loss_list),\"|\",np.mean(test_acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist(learning_rate):\n",
    "    DATA_FOLDER = \"/Users/siva/Documents/thesis/datasets/\"\n",
    "    TENSORBOARD_FOLDER = \"/Users/siva/Documents/thesis/tensorboard\"\n",
    "    EPOCHS = 100\n",
    "    BATCH_SIZE = 100\n",
    "    #loading the data\n",
    "#     for i in range(5):\n",
    "#         if i == 0:\n",
    "#             data = unpickle(DATA_FOLDER+\"data_batch_{0}\".format(i+1))[b'data']\n",
    "#             labels = unpickle(DATA_FOLDER+\"data_batch_{0}\".format(i+1))[b'labels']\n",
    "#             labels = np.array(list(map(lambda x:np.eye(10)[x],labels)))\n",
    "#         else:\n",
    "#             data = np.append(data,unpickle(DATA_FOLDER+\"data_batch_{0}\".format(i+1))[b'data'],axis = 0)\n",
    "#             labels_ = unpickle(DATA_FOLDER+\"data_batch_{0}\".format(i+1))[b'labels']\n",
    "#             labels = np.append(labels,np.array(list(map(lambda x:np.eye(10)[x],labels_))),axis = 0)\n",
    "#     test_data = unpickle(DATA_FOLDER+\"test_batch\")[b'data']\n",
    "#     test_labels = unpickle(DATA_FOLDER+\"test_batch\")[b'labels']\n",
    "#     test_labels = np.array(list(map(lambda x:np.eye(10)[x],test_labels)))\n",
    "    data = np.reshape(mnist.train.images,(-1,28,28,1))\n",
    "    labels = mnist.train.labels\n",
    "    test_data = np.reshape(mnist.test.images,(-1,28,28,1))\n",
    "    test_labels = mnist.test.labels\n",
    "    print(test_data.shape)\n",
    "    print(test_labels.shape)\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess:\n",
    "        mnist_model = model_final(3,300,1,learning_rate,[None,28,28,1])\n",
    "        filters = 256\n",
    "        mask = 0\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        train_writer = tf.summary.FileWriter(TENSORBOARD_FOLDER+\"/train\",sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(TENSORBOARD_FOLDER+\"/test\",sess.graph)\n",
    "        train_count = 0\n",
    "        test_count = 0\n",
    "        for e in range(EPOCHS):\n",
    "            train_loss_list = []\n",
    "            train_acc_list = []\n",
    "            test_loss_list = []\n",
    "            test_acc_list = []\n",
    "            for b in range(int(data.shape[0]/BATCH_SIZE)):\n",
    "                train_data = data[b:b+BATCH_SIZE,:]\n",
    "                train_labels = labels[b:b+BATCH_SIZE,:]\n",
    "                to_compute = [mnist_model[\"optimizer\"],mnist_model[\"loss\"],mnist_model[\"accuracy\"],mnist_model[\"tensor_b\"]]\n",
    "                feed_dict_ = dict(zip(mnist_model[\"inputs\"],[train_data,train_labels,filters,mask]))\n",
    "                _,train_loss,train_acc,train_summ = sess.run(to_compute,feed_dict = feed_dict_)\n",
    "                train_loss_list.append(train_loss)\n",
    "                train_acc_list.append(train_acc)\n",
    "                #train_writer.add_summary(train_summ,train_count)\n",
    "                train_count += 1\n",
    "            print(\"train:\",np.mean(train_loss_list),\"|\",np.mean(train_acc_list))\n",
    "            for t in range(int(test_data.shape[0]/BATCH_SIZE)):\n",
    "                test_data_ = test_data[t:t+BATCH_SIZE,:]\n",
    "                test_labels_ = test_labels[t:t+BATCH_SIZE,:]\n",
    "                to_compute_t = [mnist_model[\"loss\"],mnist_model[\"accuracy\"],mnist_model[\"tensor_b\"]]\n",
    "                feed_dict_t = dict(zip(mnist_model[\"inputs\"],[test_data_,test_labels_,filters,mask]))\n",
    "                test_loss,test_acc,test_summ = sess.run(to_compute_t,feed_dict = feed_dict_t)\n",
    "                test_loss_list.append(test_loss)\n",
    "                test_acc_list.append(test_acc)\n",
    "                #test_writer.add_summary(test_summ,test_count)\n",
    "                test_count += 1\n",
    "            print(\"test:\",np.mean(test_loss_list),\"|\",np.mean(test_acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******learning rate: 1e-06\n",
      "(10000, 28, 28, 1)\n",
      "(10000, 10)\n",
      "train: 2.24821 | 0.266436\n",
      "test: 2.23831 | 0.3545\n",
      "train: 2.13466 | 0.476709\n",
      "test: 2.14539 | 0.5147\n",
      "train: 1.9807 | 0.643\n",
      "test: 2.01241 | 0.5788\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-415bbc31c0e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlr_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*******learning rate: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-07ed13b3fb06>\u001b[0m in \u001b[0;36mtrain_mnist\u001b[0;34m(learning_rate)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mto_compute\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmnist_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"optimizer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmnist_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmnist_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmnist_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tensor_b\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mfeed_dict_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"inputs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_summ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_compute\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                 \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mtrain_acc_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/siva/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/siva/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/siva/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/siva/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/siva/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = np.linspace(-6,6,13)\n",
    "lr = 10**lr\n",
    "for lr_ in lr:\n",
    "    print(\"*******learning rate: {0}\".format(str(lr_)))\n",
    "    train_mnist(lr_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    #data = np.random.normal(0,1,[2,3,3,2])\n",
    "    data = np.reshape(np.arange(36),(2,3,3,2))\n",
    "    labels = np.ones((2,10))\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess:\n",
    "        my_model = model_dummy(2,10,1,1)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        w_prev = sess.run(my_model[\"weight\"],feed_dict = \\\n",
    "                          {my_model[\"inputs\"][0]:data,my_model[\"inputs\"][1]:labels,\\\n",
    "                           my_model[\"inputs\"][2]:8,my_model[\"inputs\"][3]:1})\n",
    "        print(w_prev)\n",
    "        for i in range(10):\n",
    "            to_compute = [my_model[\"output\"],my_model[\"weight\"],my_model[\"optimizer\"]]\n",
    "            feed_dict_ = {my_model[\"inputs\"][0]:data,my_model[\"inputs\"][1]:labels,my_model[\"inputs\"][2]:8,my_model[\"inputs\"][3]:1}\n",
    "            out_,w_,_ = sess.run(to_compute,feed_dict = feed_dict_)\n",
    "            #print(out_[0])\n",
    "            #print(out_)\n",
    "            print(w_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_dummy(FILTER_SIZE,MAX_FILTERS,RATE_OF_NEURONS,LEARNING_RATE,INPUT_DIM = [None,3,3,2],OUTPUT_DIM = [None,10]):\n",
    "    \"\"\"\n",
    "    FILTER_SIZE: size of the filters used in convolution\n",
    "    MAX_FILTERS: maximum number of filters initialized\n",
    "    RATE_OF_NEURONS: rate of increase of neurons\n",
    "    LEARNING_RATE: learning rate of the optimizer\n",
    "    INPUT_DIM: input dimension of placeholder x\n",
    "    OUTPUT_DIM: output dimension of placeholder y\n",
    "    \"\"\"\n",
    "    x = tf.placeholder(tf.float32,INPUT_DIM)\n",
    "    y = tf.placeholder(tf.float32,OUTPUT_DIM)\n",
    "    \n",
    "    no_of_filters = tf.placeholder(tf.int32)\n",
    "    mask = tf.placeholder(tf.int32)\n",
    "    \n",
    "    conv_w_original = weights_([FILTER_SIZE,FILTER_SIZE,3,MAX_FILTERS])\n",
    "    conv_b_original = biases_([MAX_FILTERS])\n",
    "    \n",
    "    #conv_w = tf.slice(conv_w_original,(0,0,0,0),(FILTER_SIZE,FILTER_SIZE,tf.shape(x)[-1],no_of_filters))\n",
    "    conv_w = conv_w_original[0:FILTER_SIZE,0:FILTER_SIZE,0:tf.shape(x)[-1],0:no_of_filters]\n",
    "    conv_b = conv_b_original[0:no_of_filters]\n",
    "    \n",
    "    conv_w_mask = tf.concat([tf.ones([FILTER_SIZE,FILTER_SIZE,tf.shape(x)[-1],no_of_filters-RATE_OF_NEURONS]),\\\n",
    "                             tf.zeros([FILTER_SIZE,FILTER_SIZE,tf.shape(x)[-1],RATE_OF_NEURONS])],3)\n",
    "    conv_b_mask = tf.concat([tf.ones([no_of_filters-RATE_OF_NEURONS]),tf.zeros([RATE_OF_NEURONS])],0)\n",
    "    conv_w_mask_not = tf.concat([tf.zeros([FILTER_SIZE,FILTER_SIZE,tf.shape(x)[-1],no_of_filters-RATE_OF_NEURONS]),\\\n",
    "                                 tf.ones([FILTER_SIZE,FILTER_SIZE,tf.shape(x)[-1],RATE_OF_NEURONS])],3)\n",
    "    conv_b_mask_not = tf.concat([tf.zeros([no_of_filters-RATE_OF_NEURONS]),tf.ones([RATE_OF_NEURONS])],0)\n",
    "    \n",
    "    #depending on the mask value gradient update is done\n",
    "    w = tf.cond(tf.equal(mask,1),lambda:tf.stop_gradient(conv_w * conv_w_mask) + (conv_w * conv_w_mask_not),\\\n",
    "                lambda:conv_w)\n",
    "    b = tf.cond(tf.equal(mask,1),lambda:tf.stop_gradient(conv_b * conv_b_mask) + (conv_b * conv_b_mask_not),\\\n",
    "                lambda:conv_b)\n",
    "        \n",
    "    conv_out = tf.nn.relu(tf.nn.bias_add(conv2d_(x,w),b))\n",
    "    \n",
    "    full_w_original = weights_([INPUT_DIM[1]*INPUT_DIM[2]*MAX_FILTERS,10])\n",
    "    full_w = tf.slice(full_w_original,(0,0),(INPUT_DIM[1]*INPUT_DIM[2]*no_of_filters,10))\n",
    "    \n",
    "    out = tf.matmul(tf.reshape(conv_out,(-1,INPUT_DIM[1]*INPUT_DIM[2]*no_of_filters)),full_w)\n",
    "    #softmax_out = tf.nn.softmax(out)\n",
    "    \n",
    "    loss = out - y\n",
    "    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = out,labels = y))\n",
    "    prediction = tf.equal(tf.argmax(out,axis = 1),tf.argmax(y, axis = 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(prediction,tf.float32))\n",
    "    optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "    \n",
    "    #summary writing\n",
    "    tf.summary.scalar(\"loss\",loss)\n",
    "    tf.summary.scalar(\"acc\",accuracy)\n",
    "    tensor_b = tf.summary.merge_all()\n",
    "    \n",
    "    return {\"inputs\":[x,y,no_of_filters,mask],\"weight\":b,\"optimizer\":optimizer,\"loss\":loss,\"output\":conv_out,\"accuracy\":accuracy,\"tensor_b\":tensor_b}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01502581 -0.00239594 -0.01728372 -0.02503985 -0.00987763 -0.00050823\n",
      "  0.02000872 -0.0014663 ]\n",
      "[ 0.01502581 -0.00239594 -0.01728372 -0.02503985 -0.00987763 -0.00050823\n",
      "  0.02000872 -0.0014663 ]\n",
      "[ 0.01502581 -0.00239594 -0.01728372 -0.02503985 -0.00987763 -0.00050823\n",
      "  0.02000872 -0.0014663 ]\n",
      "[ 0.01502581 -0.00239594 -0.01728372 -0.02503985 -0.00987763 -0.00050823\n",
      "  0.02000872 -0.0014663 ]\n",
      "[ 0.01502581 -0.00239594 -0.01728372 -0.02503985 -0.00987763 -0.00050823\n",
      "  0.02000872 -0.0014663 ]\n",
      "[ 0.01502581 -0.00239594 -0.01728372 -0.02503985 -0.00987763 -0.00050823\n",
      "  0.02000872 -0.0014663 ]\n",
      "[ 0.01502581 -0.00239594 -0.01728372 -0.02503985 -0.00987763 -0.00050823\n",
      "  0.02000872 -0.0014663 ]\n",
      "[ 0.01502581 -0.00239594 -0.01728372 -0.02503985 -0.00987763 -0.00050823\n",
      "  0.02000872 -0.0014663 ]\n",
      "[ 0.01502581 -0.00239594 -0.01728372 -0.02503985 -0.00987763 -0.00050823\n",
      "  0.02000872 -0.0014663 ]\n",
      "[ 0.01502581 -0.00239594 -0.01728372 -0.02503985 -0.00987763 -0.00050823\n",
      "  0.02000872 -0.0014663 ]\n",
      "[ 0.01502581 -0.00239594 -0.01728372 -0.02503985 -0.00987763 -0.00050823\n",
      "  0.02000872 -0.0014663 ]\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
